# -*- coding: utf-8 -*-
"""w1954815_FYP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11DDKkHGJ72180geDdh90KWsDCJB1qwHS
"""

from google.colab import drive
drive.mount('/content/drive')

"""/data --> file path (cd)

"""

import pandas as pd

# Step 1: Set the file path
file_path = '/content/drive/MyDrive/Dataset/FYP_Final_Data.csv'

# Step 2: Load the CSV file
df = pd.read_csv(file_path)

# Step 3: Display the first few rows
df.head()

"""# Data Preprocessing

"""

# Clean column names
df.columns = df.columns.str.strip().str.replace('\n', ' ').str.replace(r"[^\w\s]", '', regex=True)
df.columns

# Check for missing or placeholder values
missing_counts = df.isin(['', ' ', 'N/A', 'NaN', 'None']).sum()
print("Missing (placeholder) values:\n", missing_counts)

# Also check actual NaNs
print("\nActual NaNs:\n", df.isnull().sum())

df['Product'] = df['Product'].str.strip()
df['Month'] = df['Month'].str.strip()

import pandas as pd

# Convert Month names to numbers, coercing errors to NaT
df['Month_Num'] = pd.to_datetime(df['Month'], format='%b', errors='coerce').dt.month

# Combine Year and Month into a Date column, coercing errors to NaT
df['Date'] = pd.to_datetime(dict(year=df['Year'], month=df['Month_Num'], day=1), errors='coerce')

# Check for any unparsed months or dates
unparsed_months = df[df['Month_Num'].isnull()]['Month'].unique()
if len(unparsed_months) > 0:
    print(f"Warning: The following month names could not be parsed and resulted in NaNs: {list(unparsed_months)}")
    print("Consider cleaning these entries in the 'Month' column.")

unparsed_dates_count = df['Date'].isnull().sum()
if unparsed_dates_count > 0:
    print(f"Warning: {unparsed_dates_count} dates could not be fully constructed due to parsing errors or missing Year/Month data.")

# Correct the 'Sept' typo to 'Sep'
df['Month'] = df['Month'].replace('Sept', 'Sep')
print("Corrected 'Sept' to 'Sep' in the 'Month' column.")

# Re-run the date conversion after correcting the month name
df['Month_Num'] = pd.to_datetime(df['Month'], format='%b', errors='coerce').dt.month
df['Date'] = pd.to_datetime(dict(year=df['Year'], month=df['Month_Num'], day=1), errors='coerce')

unparsed_months_after_correction = df[df['Month_Num'].isnull()]['Month'].unique()
if len(unparsed_months_after_correction) > 0:
    print(f"Warning after correction: The following month names still could not be parsed: {list(unparsed_months_after_correction)}")
else:
    print("All month names parsed successfully after correction.")

unparsed_dates_count_after_correction = df['Date'].isnull().sum()
if unparsed_dates_count_after_correction > 0:
    print(f"Warning after correction: {unparsed_dates_count_after_correction} dates could not be fully constructed.")
else:
    print("All dates constructed successfully after correction.")

print("Total rows:", len(df))

df = df.drop(columns=['Month', 'Month_Num'])

df['FOB_Value_USD'] = df['FOB_value_000'] * 1000

# Safely convert to numeric just in case, then multiply
df['FOB_Value_USD'] = pd.to_numeric(df['FOB_value_000'], errors='coerce') * 1000

# Check the result
df[['FOB_value_000', 'FOB_Value_USD']].head()

df.drop(columns=['FOB_value_000'], inplace=True)

er_df = pd.read_csv('/content/drive/MyDrive/Dataset/Exchange Rate - Coconut.csv')
er_df.head()

import calendar

er_df['Month'] = er_df['Month'].apply(lambda x: calendar.month_abbr[list(calendar.month_name).index(x)])

df['Month'] = df['Date'].dt.strftime('%b')  # Jan, Feb, etc.
df['Year'] = df['Date'].dt.year

df['Year'] = df['Year'].astype(int)
df.head()

# Ensure column names of er_df are clean (strip any whitespace)
er_df.columns = er_df.columns.str.strip()

merged_df_er = pd.merge(df, er_df, on=['Year', 'Month'], how='left')

merged_df_er.head()

fresh_df = pd.read_csv('/content/drive/MyDrive/Dataset/Cleaned_Fresh_Nut_Consumption.csv')
fresh_df.head()

merged_df_fresh = pd.merge(merged_df_er, fresh_df, on=['Year', 'Month'], how='left')

merged_df_fresh.head()

# Convert Fresh_Nut_Million to actual numbers by multiplying by 1,000,000
merged_df_fresh['Fresh_Nut_Actual'] = merged_df_fresh['Fresh_Nut_Million'] * 1_000_000

# Display the head of the DataFrame to show the new column
merged_df_fresh.head()

merged_df_fresh.info()

local_price_df = pd.read_csv('/content/drive/MyDrive/Dataset/Coconut_local_price.csv')
local_price_df.head()

local_price_df.info()

merged_df_fresh_local = pd.merge(merged_df_fresh, local_price_df, on=['Year', 'Month'], how='left')
merged_df_fresh_local.head()

# Convert 'Volume' and 'Avg_FOB' to numeric, coercing errors to NaN
merged_df_fresh_local['Volume'] = pd.to_numeric(merged_df_fresh_local['Volume'], errors='coerce')
merged_df_fresh_local['Avg_FOB'] = pd.to_numeric(merged_df_fresh_local['Avg_FOB'], errors='coerce')

print("Data types after conversion:")
merged_df_fresh_local[['Volume', 'Avg_FOB', 'FOB_Value_USD']].info()

merged_df_fresh_local.head()

print("\n--- Missing Values by Variable (Overall) ---")
missing_overall = merged_df_fresh_local.isnull().sum()
print(missing_overall[missing_overall > 0])

print("\n--- Missing Values by Variable and Product ---")
missing_by_product = merged_df_fresh_local.groupby('Product').apply(lambda x: x.isnull().sum())
# Filter to show only columns with missing values for at least one product
missing_by_product = missing_by_product.loc[:, (missing_by_product != 0).any(axis=0)]

if not missing_by_product.empty:
    display(missing_by_product)
else:
    print("No missing values found when grouped by product.")

# Clean placeholders in target columns
merged_df_fresh_local[['Volume', 'Avg_FOB', 'FOB_Value_USD']] = merged_df_fresh_local[
    ['Volume', 'Avg_FOB', 'FOB_Value_USD']
].replace(['-', '', ' '], pd.NA)

# Convert columns to numeric
merged_df_fresh_local['Volume'] = pd.to_numeric(merged_df_fresh_local['Volume'], errors='coerce')
merged_df_fresh_local['Avg_FOB'] = pd.to_numeric(merged_df_fresh_local['Avg_FOB'], errors='coerce')
merged_df_fresh_local['FOB_Value_USD'] = pd.to_numeric(merged_df_fresh_local['FOB_Value_USD'], errors='coerce')

# Define the fill function

def fill_by_month_product_median(row, col):
    if pd.isna(row[col]):
        product = row['Product']
        month = row['Month']
        year = row['Year']
        median_val = merged_df_fresh_local[
            (merged_df_fresh_local['Product'] == product) &
            (merged_df_fresh_local['Month'] == month) &
            (merged_df_fresh_local['Year'] != year)
        ][col].median()
        return median_val
    else:
        return row[col]

# Fill missing values for Volume and Avg_FOB
merged_df_fresh_local['Volume'] = merged_df_fresh_local.apply(
    lambda row: fill_by_month_product_median(row, 'Volume'), axis=1
)
merged_df_fresh_local['Avg_FOB'] = merged_df_fresh_local.apply(
    lambda row: fill_by_month_product_median(row, 'Avg_FOB'), axis=1
)

# Recalculate FOB_Value_USD
merged_df_fresh_local['FOB_Value_USD'] = merged_df_fresh_local['Volume'] * merged_df_fresh_local['Avg_FOB']

# Check missing values
print(merged_df_fresh_local[['Volume', 'Avg_FOB', 'FOB_Value_USD']].isnull().sum())

merged_df_fresh_local["Date"] = pd.to_datetime(
    merged_df_fresh_local["Date"], errors="coerce"
)

merged_df_fresh_local = (
    merged_df_fresh_local
    .sort_values(["Product", "Date"])
    .reset_index(drop=True)
)

#Date and numeric columns were standardised and validated to ensure
#chronological consistency and numerical integrity prior to time-series analysis and modelling

merged_df_fresh_local["Date"] = pd.to_datetime(
    merged_df_fresh_local["Date"], errors="coerce"
)

merged_df_fresh_local = (
    merged_df_fresh_local
    .sort_values(["Product", "Date"])
    .reset_index(drop=True)
)

num_cols = [
    "Volume", "Avg_FOB", "FOB_Value_USD", "USD",
    "Fresh_Nut_Million", "Fresh_Nut_Actual", "Coconut_local_price"
]

for c in num_cols:
    merged_df_fresh_local[c] = pd.to_numeric(
        merged_df_fresh_local[c], errors="coerce"
    )

num_cols = [
    "Volume", "Avg_FOB", "FOB_Value_USD", "USD",
    "Fresh_Nut_Million", "Fresh_Nut_Actual", "Coconut_local_price"
]

for c in num_cols:
    merged_df_fresh_local[c] = pd.to_numeric(
        merged_df_fresh_local[c], errors="coerce"
    )

# fixing the missing values for coconut local price -nearby months of same product
merged_df_fresh_local["Coconut_local_price"] = (
    merged_df_fresh_local
    .groupby("Product")["Coconut_local_price"]
    .apply(lambda s: s.ffill().bfill())
    .reset_index(level=0, drop=True)
)

merged_df_fresh_local["Year"] = merged_df_fresh_local["Date"].dt.year
merged_df_fresh_local["Month"] = merged_df_fresh_local["Date"].dt.month

# Standardise product naming
merged_df_fresh_local["Product"] = (
    merged_df_fresh_local["Product"]
    .replace({
        "Dessicated Coconut": "Desiccated Coconut",
        "Desicted Coconut": "Desiccated Coconut"
    })
)

# Verify
merged_df_fresh_local["Product"].value_counts()

# Cyclical Encoding - Dec and Jan being close to each other
import numpy as np
merged_df_fresh_local["Month_sin"] = np.sin(
    2 * np.pi * merged_df_fresh_local["Month"] / 12
)

merged_df_fresh_local["Month_cos"] = np.cos(
    2 * np.pi * merged_df_fresh_local["Month"] / 12
)

# One-hot encoding for products --> cannot interpret text numbers
merged_df_preprocessed = pd.get_dummies(
    merged_df_fresh_local,
    columns=["Product"],
    prefix="Product",
    drop_first=False
)

merged_df_preprocessed = (
    merged_df_preprocessed
    .dropna(subset=["Date", "Volume"])
    .reset_index(drop=True)
)

merged_df_fresh_local[
    (merged_df_fresh_local["Year"] != merged_df_fresh_local["Date"].dt.year) |
    (merged_df_fresh_local["Month"] != merged_df_fresh_local["Date"].dt.month)
]

# ensuring that values are numerically
merged_df_fresh_local[["Month", "Month_sin", "Month_cos"]].head(12)

print(
    merged_df_fresh_local[["Month_sin", "Month_cos"]].min(),
    merged_df_fresh_local[["Month_sin", "Month_cos"]].max()
)

product_cols = [c for c in merged_df_preprocessed.columns if c.startswith("Product_")]

# Check one-hot correctness
merged_df_preprocessed[product_cols].sum(axis=1).value_counts()

print("Missing Volume:", merged_df_preprocessed["Volume"].isna().sum())
print("Negative Volume:", (merged_df_preprocessed["Volume"] < 0).sum())

merged_df_preprocessed["Volume"].describe()

print("Date type:", merged_df_fresh_local["Date"].dtype)
print("\nMissing values:\n", merged_df_fresh_local.isna().sum())

product_cols = [c for c in merged_df_preprocessed.columns if c.startswith("Product_")]
print("\nOne-hot product check:\n", merged_df_preprocessed[product_cols].sum(axis=1).value_counts())

print("\nVolume stats:\n", merged_df_preprocessed["Volume"].describe())

output_path_preprocessed = '/content/drive/MyDrive/Dataset/FYP_Preprocessed_Data.csv'
merged_df_preprocessed.to_csv(output_path_preprocessed, index=False)
print(f"DataFrame successfully saved to {output_path_preprocessed}")

"""Feature Engineering

My updated df is merged_df_preprocessed

"""

product_cols = [c for c in merged_df_preprocessed.columns if c.startswith("Product_")]
product_cols

# Sort by product & date before lagging
merged_df_preprocessed = (
    merged_df_preprocessed
    .sort_values(["Date"])
    .reset_index(drop=True)
)

# Create lag features
merged_df_preprocessed["Volume_lag_1"] = merged_df_preprocessed["Volume"].shift(1)
merged_df_preprocessed["Volume_lag_3"] = merged_df_preprocessed["Volume"].shift(3)

merged_df_preprocessed["USD_lag_1"] = merged_df_preprocessed["USD"].shift(1)
merged_df_preprocessed["Avg_FOB_lag_1"] = merged_df_preprocessed["Avg_FOB"].shift(1)

# Rolling averages (short & medium term trends)
merged_df_preprocessed["Volume_3m_avg"] = (
    merged_df_preprocessed["Volume"]
    .rolling(window=3)
    .mean()
)

merged_df_preprocessed["Avg_FOB_6m_avg"] = (
    merged_df_preprocessed["Avg_FOB"]
    .rolling(window=6)
    .mean()
)

# Rolling volatility (market instability)
merged_df_preprocessed["USD_3m_std"] = (
    merged_df_preprocessed["USD"]
    .rolling(window=3)
    .std()
)

merged_df_preprocessed["USD_x_AvgFOB"] = (
    merged_df_preprocessed["USD"] * merged_df_preprocessed["Avg_FOB"]
)

# Drop initial rows where lag/rolling features are NaN
merged_df_fe = merged_df_preprocessed.dropna().reset_index(drop=True)

merged_df_fe.shape

merged_df_fe.info()
merged_df_fe.head()

# Identify product columns
product_cols = [c for c in merged_df_preprocessed.columns if c.startswith("Product_")]

# Reconstruct Product label from one-hot encoding
merged_df_preprocessed["Product"] = (
    merged_df_preprocessed[product_cols]
    .idxmax(axis=1)
)

# Sort before lagging (important)
merged_df_preprocessed = (
    merged_df_preprocessed
    .sort_values(["Product", "Date"])
    .reset_index(drop=True)
)

# Product-wise lag
merged_df_preprocessed["Volume_lag_1"] = (
    merged_df_preprocessed
    .groupby("Product")["Volume"]
    .shift(1)
)

merged_df_preprocessed[
    ["Product", "Date", "Volume", "Volume_lag_1"]
].head(10)

merged_df_fe = merged_df_preprocessed.dropna().reset_index(drop=True)
merged_df_fe.shape

merged_df_preprocessed.groupby("Product").size()
merged_df_fe.groupby("Product").size()

"""# Exploratory Data Analysis

"""

merged_df_fe.head()

print("Dataset shape:", merged_df_fe.shape)
print("\nColumns:\n", merged_df_fe.columns.tolist())

merged_df_fe.isna().sum().sort_values(ascending=False)

merged_df_fe["Volume"].describe()

print("Negative volumes:", (merged_df_fe["Volume"] < 0).sum())
print("Zero volumes:", (merged_df_fe["Volume"] == 0).sum())

volume_stats = (
    merged_df_fe
    .groupby("Product")["Volume"]
    .agg(["mean", "std", "min", "max", "median"])
    .sort_values("mean", ascending=False)
)

volume_stats

volume_stats["cv"] = volume_stats["std"] / volume_stats["mean"]
volume_stats.sort_values("cv", ascending=False)

"""## Volume Behaviour Across Products

"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.boxplot(
    data=merged_df_fe,
    x="Product",
    y="Volume"
)
plt.xticks(rotation=45)
plt.title("Export Volume Distribution by Product")
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

sample_products = merged_df_fe["Product"].unique()[:4]

for p in sample_products:
    subset = merged_df_fe[merged_df_fe["Product"] == p]

    plt.figure(figsize=(6, 4))
    sns.boxplot(y=subset["Volume"])
    plt.title(f"Volume Outliers – {p}")
    plt.ylabel("Volume")
    plt.show()

from scipy.stats import zscore

merged_df_fe["volume_z"] = (
    merged_df_fe
    .groupby("Product")["Volume"]
    .transform(lambda x: zscore(x, nan_policy="omit"))
)

# Flag potential outliers
outliers = merged_df_fe[merged_df_fe["volume_z"].abs() > 3]

outliers[["Product", "Date", "Volume", "volume_z"]].head()

# Function to detect IQR outliers for one product

def detect_iqr_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return df[(df[col] < lower) | (df[col] > upper)]

# Detect outliers for all products
outliers_df = (
    merged_df_fe
    .groupby("Product", group_keys=False)
    .apply(lambda x: detect_iqr_outliers(x, "Volume"))
)

outliers_df[["Product", "Date", "Volume"]].head()

products = merged_df_fe["Product"].unique()

for p in products:
    subset = merged_df_fe[merged_df_fe["Product"] == p]
    p_outliers = outliers_df[outliers_df["Product"] == p]

    plt.figure(figsize=(12, 4))
    plt.plot(subset["Date"], subset["Volume"], label="Volume", linewidth=2)

    if not p_outliers.empty:
        plt.scatter(
            p_outliers["Date"],
            p_outliers["Volume"],
            color="red",
            label="Flagged outliers",
            zorder=3
        )

    plt.title(f"Export Volume Time Series with Outliers – {p}")
    plt.xlabel("Date")
    plt.ylabel("Volume")
    plt.legend()
    plt.tight_layout()
    plt.show()

sample_products = merged_df_fe["Product"].unique()[:4]

plt.figure(figsize=(12, 6))
for p in sample_products:
    subset = merged_df_fe[merged_df_fe["Product"] == p]
    plt.plot(subset["Date"], subset["Volume"], label=p)

plt.legend()
plt.title("Export Volume Over Time (Sample Products)")
plt.xlabel("Date")
plt.ylabel("Volume")
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))

for p in sample_products:
    subset = merged_df_fe[merged_df_fe["Product"] == p].copy()
    subset["rolling_std"] = subset["Volume"].rolling(6).std()
    plt.plot(subset["Date"], subset["rolling_std"], label=p)

plt.legend()
plt.title("6-Month Rolling Volatility of Export Volume")
plt.xlabel("Date")
plt.ylabel("Rolling Std of Volume")
plt.tight_layout()
plt.show()

"""I decided to keep them because:


*   Covid-19
*   Supply Shocks
*   Policy changes
*   Sudden demand surges

Removing them would hide real-world behaviour

Models like XGBoost can handle such variations

## TEMPORAL TRENDS

"""

total_volume_ts = (
    merged_df_fe
    .groupby("Date")["Volume"]
    .sum()
)

plt.figure(figsize=(12, 5))
plt.plot(total_volume_ts.index, total_volume_ts.values)
plt.title("Total Export Volume Over Time (All Products)")
plt.xlabel("Date")
plt.ylabel("Total Export Volume")
plt.tight_layout()
plt.show()

rolling_12m = total_volume_ts.rolling(12).mean()

plt.figure(figsize=(12, 5))
plt.plot(total_volume_ts.index, total_volume_ts.values, label="Monthly Volume")
plt.plot(rolling_12m.index, rolling_12m.values, label="12-Month Rolling Average")
plt.title("Overall Export Volume Trend (Smoothed)")
plt.xlabel("Date")
plt.ylabel("Total Export Volume")
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 5))

for p in sample_products:
    subset = merged_df_fe[merged_df_fe["Product"] == p].copy()
    subset["rolling_mean"] = subset["Volume"].rolling(6).mean()
    plt.plot(subset["Date"], subset["rolling_mean"], label=p)

plt.title("6-Month Rolling Export Volume Trends (Sample Products)")
plt.xlabel("Date")
plt.ylabel("Rolling Mean Volume")
plt.legend()
plt.tight_layout()
plt.show()

"""## Seasonality Analysis

"""

monthly_avg = (
    merged_df_fe
    .groupby("Month")["Volume"]
    .mean()
)

plt.figure(figsize=(10, 4))
plt.plot(monthly_avg.index, monthly_avg.values, marker="o")
plt.title("Average Monthly Export Volume (All Products)")
plt.xlabel("Month")
plt.ylabel("Average Volume")
plt.xticks(range(1, 13))
plt.grid(True)
plt.tight_layout()
plt.show()

product_monthly_avg = (
    merged_df_fe
    .groupby(["Product", "Month"])["Volume"]
    .mean()
    .reset_index()
)

product_monthly_avg.head()

pivot_table = product_monthly_avg.pivot(
    index="Product",
    columns="Month",
    values="Volume"
)

plt.figure(figsize=(12, 6))
sns.heatmap(
    pivot_table,
    cmap="YlGnBu",
    linewidths=0.5
)
plt.title("Monthly Export Volume Seasonality by Product")
plt.xlabel("Month")
plt.ylabel("Product")
plt.tight_layout()
plt.show()

seasonality_strength = (
    product_monthly_avg
    .groupby("Product")["Volume"]
    .agg(lambda x: x.max() - x.min())
    .sort_values(ascending=False)
)

seasonality_strength

seasonality_cv = (
    product_monthly_avg
    .groupby("Product")["Volume"]
    .apply(lambda x: (x.max() - x.min()) / x.mean())
    .sort_values(ascending=False)
)

seasonality_cv

"""## Relationship between Variables

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

overall_corr = merged_df_fe[
    ["Volume", "USD", "Avg_FOB", "Fresh_Nut_Actual"]
].corr()

overall_corr

plt.figure(figsize=(6, 4))

sns.heatmap(
    overall_corr,
    annot=True,          # show correlation values
    fmt=".2f",           # 2 decimal places
    cmap="coolwarm",     # red–blue colour scale
    center=0,            # zero-centered colour
    linewidths=0.5
)

plt.title("Correlation Heatmap of Key Variables")
plt.tight_layout()
plt.show()

product_corr = (
    merged_df_fe
    .groupby("Product")
    .apply(lambda x: pd.Series({
        "corr_USD": x["Volume"].corr(x["USD"]),
        "corr_Avg_FOB": x["Volume"].corr(x["Avg_FOB"]),
        "corr_Supply": x["Volume"].corr(x["Fresh_Nut_Actual"])
    }))
)

product_corr

plt.figure(figsize=(8, 6))
sns.heatmap(
    product_corr,
    annot=True,
    cmap="coolwarm",
    center=0
)
plt.title("Product-wise Correlation of Volume with Key Drivers")
plt.tight_layout()
plt.show()

sample_products = merged_df_fe["Product"].unique()[:3]

for p in sample_products:
    subset = merged_df_fe[merged_df_fe["Product"] == p]

    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    axes[0].scatter(subset["USD"], subset["Volume"])
    axes[0].set_title(f"{p}: Volume vs USD")

    axes[1].scatter(subset["Avg_FOB"], subset["Volume"])
    axes[1].set_title(f"{p}: Volume vs Avg_FOB")

    axes[2].scatter(subset["Fresh_Nut_Actual"], subset["Volume"])
    axes[2].set_title(f"{p}: Volume vs Supply")

    plt.tight_layout()
    plt.show()

p = merged_df_fe["Product"].unique()[0]
subset = merged_df_fe[merged_df_fe["Product"] == p].copy()

subset["rolling_corr_usd"] = (
    subset["Volume"]
    .rolling(12)
    .corr(subset["USD"])
)

plt.figure(figsize=(10, 4))
plt.plot(subset["Date"], subset["rolling_corr_usd"])
plt.title(f"12-Month Rolling Correlation: Volume vs USD ({p})")
plt.xlabel("Date")
plt.ylabel("Rolling Correlation")
plt.tight_layout()
plt.show()

output_path_fe = '/content/drive/MyDrive/Dataset/FYP_Feature_Engineered_Data.csv'
merged_df_fe.to_csv(output_path_fe, index=False)
print(f"DataFrame successfully saved to {output_path_fe}")

# volume distribution -
merged_df_fe.groupby("Product")["Volume"].skew().sort_values(ascending=False)
sample_products = merged_df_fe["Product"].unique()[:3]

for p in sample_products:
    subset = merged_df_fe[merged_df_fe["Product"] == p]
    sns.histplot(subset["Volume"], bins=30, kde=True)
    plt.title(f"Volume Distribution – {p}")
    plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf

# Aggregate total export volume per month
ts_total = (
    merged_df_fe
    .groupby("Date")["Volume"]
    .sum()
    .sort_index()
)

# orange line shows the long term direction --> exportts increaseing with time
plt.figure(figsize=(12, 4))
plt.plot(ts_total, label="Monthly Export Volume")
plt.plot(ts_total.rolling(12).mean(), label="12-Month Moving Average", linewidth=2)
plt.title("Overall Export Volume Trend")
plt.xlabel("Date")
plt.ylabel("Volume")
plt.legend()
plt.tight_layout()
plt.show()

# Volume - raw export data
# trend - underlying long term movement
# seasonal - repeating patterns every year
# residual - after removing trends and seasonality
decomp = seasonal_decompose(ts_total, model="additive", period=12)

decomp.plot()
plt.tight_layout()
plt.show()

# Remove trend using rolling mean --> removing long term trend
trend = ts_total.rolling(12).mean()
detrended = ts_total - trend

plt.figure(figsize=(12, 4))
plt.plot(detrended)
plt.title("Detrended Export Volume (Cyclical Fluctuations)")
plt.xlabel("Date")
plt.ylabel("Detrended Volume")
plt.tight_layout()
plt.show()

# export volumes are predictable from last month
pd.plotting.lag_plot(ts_total)
plt.title("Lag Plot: Volume vs Previous Month")
plt.show()

# todays export depending on previous months exports
plt.figure(figsize=(10, 4))
plot_acf(ts_total, lags=24)
plt.title("Autocorrelation Function (ACF) – Export Volume")
plt.tight_layout()
plt.show()
# each bar - correlation with past month
# slowly decreasing correlation over time

for p in merged_df_fe["Product"].unique():
    ts = (
        merged_df_fe[merged_df_fe["Product"] == p]
        .set_index("Date")["Volume"]
        .sort_index()
    )

    plt.figure(figsize=(10, 3))
    plt.plot(ts, label="Volume")
    plt.plot(ts.rolling(12).mean(), linewidth=2, label="12M MA")
    plt.title(f"Trend – {p}")
    plt.legend()
    plt.tight_layout()
    plt.show()

from statsmodels.graphics.tsaplots import plot_acf

for p in merged_df_fe["Product"].unique():
    ts = (
        merged_df_fe[merged_df_fe["Product"] == p]
        .set_index("Date")["Volume"]
        .sort_index()
    )

    plt.figure(figsize=(8, 3))
    plot_acf(ts, lags=24)
    plt.title(f"ACF – {p}")
    plt.tight_layout()
    plt.show()

"""# Clustering

"""

import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

# ---------- Safety: Date, Product ----------
merged_df_fe["Date"] = pd.to_datetime(merged_df_fe["Date"])

product_cols = [c for c in merged_df_fe.columns if c.startswith("Product_")]
merged_df_fe["Product"] = (
    merged_df_fe[product_cols]
    .idxmax(axis=1)
    .str.replace("Product_", "")
)

merged_df_fe = merged_df_fe.sort_values(["Product", "Date"]).reset_index(drop=True)
merged_df_fe["Month"] = merged_df_fe["Date"].dt.month

from sklearn.linear_model import LinearRegression

# --- 1) Relative volatility (CV) ---
vol_stats = merged_df_fe.groupby("Product")["Volume"].agg(["mean", "std"])
vol_stats["vol_cv"] = vol_stats["std"] / vol_stats["mean"]

# --- 2) Seasonality strength (unit-safe) ---
merged_df_fe["Month"] = merged_df_fe["Date"].dt.month

product_monthly_avg = (
    merged_df_fe.groupby(["Product", "Month"])["Volume"]
    .mean()
    .reset_index()
)

seasonality_cv = (
    product_monthly_avg
    .groupby("Product")["Volume"]
    .apply(lambda x: (x.max() - x.min()) / x.mean())
)

# --- 3) Driver sensitivities (correlations) ---
product_corr = (
    merged_df_fe
    .groupby("Product")
    .apply(lambda x: pd.Series({
        "corr_USD": x["Volume"].corr(x["USD"]),
        "corr_Avg_FOB": x["Volume"].corr(x["Avg_FOB"]),
        "corr_Supply": x["Volume"].corr(x["Fresh_Nut_Actual"])
    }))
)

# --- 4) Normalized trend strength ---

def normalized_trend_strength(df):
    df = df.sort_values("Date")
    t = np.arange(len(df)).reshape(-1, 1)
    y = df["Volume"].values
    if len(y) < 5 or np.isnan(y).all():
        return np.nan
    model = LinearRegression()
    model.fit(t, y)
    slope = model.coef_[0]
    mean_vol = np.mean(y)
    return np.abs(slope) / mean_vol if mean_vol != 0 else np.nan

trend_strength = (
    merged_df_fe.groupby("Product")
    .apply(normalized_trend_strength)
    .rename("trend_strength")
)

# --- Combine into cluster_df ---
cluster_df = pd.concat([
    vol_stats[["vol_cv"]],
    seasonality_cv.rename("seasonality_cv"),
    product_corr,
    trend_strength
], axis=1).reset_index()

cluster_df

# Features for clustering
feature_cols = ["vol_cv", "seasonality_cv", "corr_USD", "corr_Avg_FOB", "corr_Supply", "trend_strength"]

# Clean + scale
cluster_df_clean = cluster_df.dropna(subset=feature_cols).copy()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(cluster_df_clean[feature_cols])

cluster_df_clean

cluster_df_clean[["trend_strength"]].describe()

def trend_growth_rate(ts):
    ts = ts.dropna()
    if len(ts) < 24:
        return np.nan
    start = ts.iloc[:12].mean()
    end = ts.iloc[-12:].mean()
    return (end - start) / start

trend_growth = (
    merged_df_fe
    .groupby("Product")["Volume"]
    .apply(trend_growth_rate)
    .reset_index(name="trend_growth_rate")
)

print(trend_growth)

"""Elbow Methods

"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

wcss = []

K_range = range(1, 8)  # 1 to 7 clusters (enough for 12 products)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)  # inertia = WCSS

# Plot elbow curve
plt.figure(figsize=(6, 4))
plt.plot(K_range, wcss, marker='o')
plt.xlabel("Number of clusters (k)")
plt.ylabel("Within-Cluster Sum of Squares (WCSS)")
plt.title("Elbow Method for Optimal k (K-Means)")
plt.tight_layout()
plt.show()

!pip install kneed

from kneed import KneeLocator

kl = KneeLocator(
    list(K_range),
    wcss,
    curve="convex",
    direction="decreasing"
)

elbow_k = kl.elbow
elbow_k

import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
plt.plot(K_range, wcss, marker="o", label="WCSS")
plt.axvline(elbow_k, color="red", linestyle="--", label=f"Elbow at k={elbow_k}")
plt.xlabel("Number of clusters (k)")
plt.ylabel("WCSS")
plt.title("Elbow Method with Automated Knee Detection")
plt.legend()
plt.tight_layout()
plt.show()

"""## K-Means

"""

from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# -------------------------
# 1) KMEANS (k = 2..6)
# -------------------------
kmeans_results = []
for k in range(2, 7):
    model = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = model.fit_predict(X_scaled)
    kmeans_results.append({
        "method": "KMeans",
        "k": k,
        "silhouette": silhouette_score(X_scaled, labels),
        "davies_bouldin": davies_bouldin_score(X_scaled, labels),
        "calinski_harabasz": calinski_harabasz_score(X_scaled, labels)
    })
kmeans_results = pd.DataFrame(kmeans_results)

"""## Hierachical Clustering

"""

hier_results = []
for k in range(2, 7):
    model = AgglomerativeClustering(n_clusters=k, linkage="ward")
    labels = model.fit_predict(X_scaled)
    hier_results.append({
        "method": "Hierarchical",
        "k": k,
        "silhouette": silhouette_score(X_scaled, labels),
        "davies_bouldin": davies_bouldin_score(X_scaled, labels),
        "calinski_harabasz": calinski_harabasz_score(X_scaled, labels)
    })
hier_results = pd.DataFrame(hier_results)

"""## Gaussian Mixture

"""

gmm_results = []
for k in range(2, 7):
    model = GaussianMixture(n_components=k, random_state=42)
    labels = model.fit_predict(X_scaled)
    gmm_results.append({
        "method": "GMM",
        "k": k,
        "silhouette": silhouette_score(X_scaled, labels),
        "davies_bouldin": davies_bouldin_score(X_scaled, labels),
        "calinski_harabasz": calinski_harabasz_score(X_scaled, labels),
        "bic": model.bic(X_scaled),
        "aic": model.aic(X_scaled)
    })
gmm_results = pd.DataFrame(gmm_results)

"""## Db Scan

"""

dbscan_results = []
for eps in [0.8, 1.0, 1.2, 1.4, 1.6]:
    model = DBSCAN(eps=eps, min_samples=2)
    labels = model.fit_predict(X_scaled)
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

    row = {"method": "DBSCAN", "eps": eps, "clusters": n_clusters, "noise_points": int((labels == -1).sum())}

    # score only if DBSCAN produced >=2 clusters and not all noise
    if n_clusters >= 2 and (labels != -1).sum() > 0:
        mask = labels != -1
        row.update({
            "silhouette": silhouette_score(X_scaled[mask], labels[mask]),
            "davies_bouldin": davies_bouldin_score(X_scaled[mask], labels[mask]),
            "calinski_harabasz": calinski_harabasz_score(X_scaled[mask], labels[mask])
        })
    dbscan_results.append(row)

dbscan_results = pd.DataFrame(dbscan_results)

"""Silhouette Score - highest value
Davies Bouldign - Lowest values
Calinski - Highest score

"""

all_results = pd.concat(
    [kmeans_results, hier_results, gmm_results],
    ignore_index=True
).sort_values(["silhouette"], ascending=False)

print("=== Top results (KMeans/Hierarchical/GMM) by Silhouette ===")
display(all_results.head(10))

print("\n=== DBSCAN results (various eps) ===")
display(dbscan_results.sort_values(["clusters", "noise_points"], ascending=[False, True]).head(10))

best_k = int(kmeans_results.sort_values("silhouette", ascending=False).iloc[0]["k"])

final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
cluster_df_clean["cluster_kmeans"] = final_kmeans.fit_predict(X_scaled)

cluster_df_clean[["Product", "cluster_kmeans"]].sort_values("cluster_kmeans")

final_hier = AgglomerativeClustering(n_clusters=best_k, linkage="ward")
cluster_df_clean["cluster_hier"] = final_hier.fit_predict(X_scaled)

final_gmm = GaussianMixture(n_components=best_k, random_state=42)
cluster_df_clean["cluster_gmm"] = final_gmm.fit_predict(X_scaled)

cluster_df_clean[["Product", "cluster_kmeans", "cluster_hier", "cluster_gmm"]].sort_values("cluster_kmeans")

from sklearn.cluster import KMeans

kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=10)
cluster_df_clean["cluster_kmeans_4"] = kmeans_4.fit_predict(X_scaled)

product_cluster_map = (
    cluster_df_clean[["Product", "cluster_kmeans_4"]]
    .sort_values("cluster_kmeans_4")
    .reset_index(drop=True)
)

product_cluster_map

cluster_composition = (
    cluster_df_clean
    .groupby("cluster_kmeans_4")["Product"]
    .apply(list)
)

cluster_composition

cluster_features = [
    "vol_cv",
    "seasonality_cv",
    "corr_USD",
    "corr_Avg_FOB",
    "corr_Supply",
    "trend_strength"
]
centroid_table = (
    cluster_df_clean
    .groupby("cluster_kmeans_4")[cluster_features]
    .mean()
    .round(3)
)

centroid_table

import numpy as np
import matplotlib.pyplot as plt

def plot_radar(centroids, features, title):
    num_vars = len(features)
    angles = np.linspace(0, 2*np.pi, num_vars, endpoint=False).tolist()
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(7, 7), subplot_kw=dict(polar=True))

    for cluster in centroids.index:
        values = centroids.loc[cluster].tolist()
        values += values[:1]
        ax.plot(angles, values, linewidth=2, label=f"Cluster {cluster}")
        ax.fill(angles, values, alpha=0.1)

    ax.set_thetagrids(np.degrees(angles[:-1]), features)
    ax.set_title(title, size=14, pad=20)
    ax.legend(loc="upper right", bbox_to_anchor=(1.3, 1.1))
    plt.tight_layout()
    plt.show()

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
centroids_scaled = pd.DataFrame(
    scaler.fit_transform(centroid_table),
    columns=centroid_table.columns,
    index=centroid_table.index
)

plot_radar(
    centroids_scaled,
    cluster_features,
    title="Behavioural Profiles of Product Clusters (k = 4)"
)

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

pca_df = pd.DataFrame(
    X_pca,
    columns=["PC1", "PC2"]
)

pca_df["cluster"] = cluster_df_clean["cluster_kmeans_4"].values
pca_df["Product"] = cluster_df_clean["Product"].values

pca.explained_variance_ratio_

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(7, 5))
sns.scatterplot(
    data=pca_df,
    x="PC1", y="PC2",
    hue="cluster",
    s=120
)

for i, row in pca_df.iterrows():
    plt.text(row["PC1"] + 0.02, row["PC2"], row["Product"], fontsize=9)

plt.title("PCA Projection of Product Clusters (k=4)")
plt.tight_layout()
plt.show()

"""Cluster 0 - driven more by market conditions and trends than by seasonality.

Cluster 01 - These products show growth + some seasonality, but are less volatile than raw products.

Cluster 02 - behaves very differently

Cluster 03 - These products have unique demand, processing, and pricing behaviour.

"""

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import StandardScaler

def gap_statistic(X, k_range, n_refs=20):
    gaps = []
    for k in k_range:
        ref_disps = []
        for _ in range(n_refs):
            random_ref = np.random.random_sample(size=X.shape)
            km = KMeans(n_clusters=k, n_init=10, random_state=42)
            km.fit(random_ref)
            ref_disps.append(km.inertia_)

        km = KMeans(n_clusters=k, n_init=10, random_state=42)
        km.fit(X)
        orig_disp = km.inertia_

        gap = np.log(np.mean(ref_disps)) - np.log(orig_disp)
        gaps.append(gap)
    return gaps

k_vals = range(2, 8)
gap_vals = gap_statistic(X_scaled, k_vals)

gap_df = pd.DataFrame({"k": list(k_vals), "gap": gap_vals})
gap_df

from sklearn.metrics import adjusted_rand_score

def bootstrap_stability(X, n_clusters, n_bootstraps=50):
    base_labels = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit_predict(X)
    scores = []

    for _ in range(n_bootstraps):
        idx = np.random.choice(len(X), len(X), replace=True)
        X_sample = X[idx]

        labels = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit_predict(X_sample)

        # Compare only overlapping indices
        score = adjusted_rand_score(base_labels[idx], labels)
        scores.append(score)

    return np.mean(scores), np.std(scores)

mean_stability, std_stability = bootstrap_stability(X_scaled, n_clusters=4)
mean_stability, std_stability

# Aggregate by cluster and date

# Create a mapping from Product to cluster_kmeans_4
product_to_cluster = cluster_df_clean.set_index('Product')['cluster_kmeans_4'].to_dict()

# Map the clusters back to merged_df_fe
merged_df_fe['Cluster'] = merged_df_fe['Product'].map(product_to_cluster)

# Define cluster names using the cluster_composition from earlier cells
cluster_names = {
    0: ', '.join(cluster_composition[0]).replace('Product_', ''),
    1: ', '.join(cluster_composition[1]).replace('Product_', ''),
    2: ', '.join(cluster_composition[2]).replace('Product_', ''),
    3: ', '.join(cluster_composition[3]).replace('Product_', '')
}

# Add Cluster_Name to merged_df_fe
merged_df_fe['Cluster_Name'] = merged_df_fe['Cluster'].map(cluster_names)


cluster_ts = (
    merged_df_fe
    .groupby(['Date', 'Cluster', 'Cluster_Name'])
    .agg({
        'Volume': 'sum',
        'FOB_Value_USD': 'sum'
    })
    .reset_index()
)

# Plot Volume by Cluster Over Time
plt.figure(figsize=(14, 6))

for cluster in range(4):
    subset = cluster_ts[cluster_ts['Cluster'] == cluster]
    cluster_name = subset['Cluster_Name'].iloc[0]
    plt.plot(subset['Date'], subset['Volume'],
             label=f'Cluster {cluster}: {cluster_name}', linewidth=2)

plt.title('Export Volume by Cluster Over Time', fontsize=14, fontweight='bold')
plt.xlabel('Date')
plt.ylabel('Total Volume')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Plot FOB Value by Cluster Over Time
plt.figure(figsize=(14, 6))

for cluster in range(4):
    subset = cluster_ts[cluster_ts['Cluster'] == cluster]
    cluster_name = subset['Cluster_Name'].iloc[0]
    plt.plot(subset['Date'], subset['FOB_Value_USD'],
             label=f'Cluster {cluster}: {cluster_name}', linewidth=2)

plt.title('Export Value (USD) by Cluster Over Time', fontsize=14, fontweight='bold')
plt.xlabel('Date')
plt.ylabel('Total FOB Value (USD)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Cluster contribution over time (stacked area chart)
pivot_volume = cluster_ts.pivot(index='Date', columns='Cluster', values='Volume').fillna(0)

plt.figure(figsize=(14, 6))
plt.stackplot(pivot_volume.index,
              *[pivot_volume[i] for i in range(4)],
              labels=[f'Cluster {i}: {cluster_names[i]}' for i in range(4)],
              alpha=0.8)
plt.title('Cluster Contribution to Total Export Volume', fontsize=14, fontweight='bold')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()